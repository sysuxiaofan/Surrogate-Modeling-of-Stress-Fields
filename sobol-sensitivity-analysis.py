# load libs
import os
import scipy.io
import numpy as np
import pandas as pd
from SALib.sample import sobol
import matplotlib.pyplot as plt
#%%
#Generate rock physical parameters using sobel sampling method
#
# define the number of parameters
D = 12
# define a dictionary to store rock physical parameters
# num_vars -> D (the number of parameters)
# names -> names of the rock physical parameters
# bounds -> ranges of the rock physical parameters
rock_physics = {
    'num_vars': D,
    'names': ['Nu_Dol', 'Nu_Ss', 'Nu_Lst', 'Nu_Fz', 'E_Dol', 'E_Ss', 'E_Lst', 'E_Fz', 'Rho_Dol', 'Rho_Ss', 'Rho_Lst', 'Rho_Fz'],
    'bounds': [[0.20, 0.35], 
               [0.20, 0.30], 
               [0.20, 0.35], 
               [0.30, 0.35], 
               [4.0e10, 8.0e10], 
               [5.0e9, 8.0e10], 
               [1.0e10, 8.0e10], 
               [5.0e9, 4.0e10], 
               [2.27e3, 2.84e3], 
               [2.12e3, 2.69e3], 
               [2.00e3, 2.65e3], 
               [1.50e3, 2.20e3]]
}

# generate Sobol sequence
# N is the number of base samples, samples = N * (2D + 2)
param_values = sobol.sample(rock_physics, N = 25, seed = 1)  # 650 samples
# save samples to a .mat file for loading to MATLAB
scipy.io.savemat('.../sobol_samples.mat', {'sobol_samples': param_values})
# show the size of parameter matrix
print(param_values.shape)
#%%
# Complie data
# The folder path that store the csv files generated by COMSOL simulations
folder_path='..../simulations/'
# Get all the csv files in the folder
csv_files = [f for f in os.listdir(folder_path) 
             if f.endswith('.csv') and os.path.isfile(os.path.join(folder_path, f))]
# Sort the csv files
csv_files.sort()
# Define arrays to store mean values of the simulated fields for sensitivity analysis
size = len(csv_files)
mean_Sigma_11 = np.zeros(size)
mean_Sigma_22 = np.zeros(size)
mean_Sigma_12 = np.zeros(size)
# read the csv files one by one
for i, filename in enumerate(csv_files, 1):
    file_path = os.path.join(folder_path, filename)
    try:
        # read a csv file
        df = pd.read_csv(file_path)
        # calculate the mean value for each field
        mean_Sigma_11[i-1]   = df['Sigma_11'].mean()
        mean_Sigma_22[i-1]   = df['Sigma_22'].mean()
        mean_Sigma_12[i-1]   = df['Sigma_12'].mean()
        print(f"\n The {i}th csv files have been finished processing！")
    except Exception as e:
        print(f"\nProcessing file {filename} error: {str(e)}")
# merge to a matrix
simulations = np.hstack(mean_Sigma_11.reshape(-1, 1), mean_Sigma_22.reshape(-1, 1), mean_Sigma_12.reshape(-1, 1))
# save to a table and csv file
df_simulations = pd.DataFrame(simulations, columns=['Sigma_11', 'Sigma_22', 'Sigma_12'])
df_simulations.to_csv(".../simulations.csv", index = False)
#%%
# Sensitivity analysis
from SALib.analyze.sobol import analyze
import pandas as pd
# load simulation results
simulations = pd.read_csv(".../simulations.csv")
# select data
Y = simulations.values
#
sensitivity_results = []
s1_results = []
st_results = []
for f in range(11):   # iterate through 11 output fields
    Si = analyze(rock_physics, Y[:, f], print_to_console = False, n_bootstrap = 1000, parallel = True)
    sensitivity_results.append(Si)
    s1_results.append(Si['S1'])
    st_results.append(Si['ST'])
#
field_names = ['Sigma_11', 'Sigma_22', 'Sigma_12']
# create Excel file
print("Saving results to Excel...")
with pd.ExcelWriter('..../sensitivity_analysis_results.xlsx', engine='xlsxwriter') as writer:
    # create table
    summary_data = []
    for field_idx, Si in enumerate(sensitivity_results):
        # find the top three sensitivity parameters
        top3_indices = np.argsort(Si['ST'])[-3:][::-1]
        top3_params = [rock_physics['names'][i] for i in top3_indices]
        top3_values = [Si['ST'][i] for i in top3_indices]
        
        summary_data.append({
            'Field': f'{field_names[field_idx]}',
            'Most Sensitive Param 1': f"{top3_params[0]} ({top3_values[0]:.3f})",
            'Most Sensitive Param 2': f"{top3_params[1]} ({top3_values[1]:.3f})",
            'Most Sensitive Param 3': f"{top3_params[2]} ({top3_values[2]:.3f})",
            'Average ST': np.mean(Si['ST'])
        })
    
    summary_df = pd.DataFrame(summary_data)
    summary_df.to_excel(writer, sheet_name='Summary', index=False)
    
    # create detail table for each field
    for field_idx, Si in enumerate(sensitivity_results):
        # create DataFrame to save results
        results_df = pd.DataFrame({
            'Parameter': rock_physics['names'],
            'First-Order (S1)': Si['S1'],
            'S1 Confidence': Si['S1_conf'],
            'Total-Order (ST)': Si['ST'],
            'ST Confidence': Si['ST_conf']
        })
        
        # ranking sensistivity values by using ST
        results_df = results_df.sort_values(by='Total-Order (ST)', ascending=False)
        
        # save to Excel
        sheet_name = f'{field_names[field_idx]}'
        results_df.to_excel(writer, sheet_name=sheet_name, index=False)
        
        # get sheet and table
        workbook = writer.book
        worksheet = writer.sheets[sheet_name]
        
        # set colum width
        worksheet.set_column('A:A', 20)   # the coloum of parameters
        worksheet.set_column('B:E', 15)   # the coloum of values
        
        # add formate - paiting color
        worksheet.conditional_format(
            f'D2:D{len(results_df)+1}',
            {
                'type': '3_color_scale',
                'min_color': '#63BE7B',  # green
                'mid_color': '#FFEB84',  # orange
                'max_color': '#F8696B'   # red
            }
        )
        
        # add title
        header_format = workbook.add_format({
            'bold': True,
            'text_wrap': True,
            'valign': 'top',
            'fg_color': '#D7E4BC',
            'border': 1
        })
        
        for col_num, value in enumerate(results_df.columns.values):
            worksheet.write(0, col_num, value, header_format)
    
    # formate summary table
    summary_sheet = writer.sheets['Summary']
    summary_sheet.set_column('A:A', 15)
    summary_sheet.set_column('B:D', 30)
    summary_sheet.set_column('E:E', 15)
    
    for col_num, value in enumerate(summary_df.columns.values):
        summary_sheet.write(0, col_num, value, header_format)
print("Analysis complete! Results saved to 'sensitivity_analysis_results.xlsx'")
#
# create DataFrame
s1_df = pd.DataFrame(s1_results, index=[f'Field_{i+1}' for i in range(11)])
st_df = pd.DataFrame(st_results, index=[f'Field_{i+1}' for i in range(11)])

# merge and save to CSV
result_df = pd.DataFrame()
for i in range(11):
    result_df[f'{field_names[i]}_S1'] = s1_df.iloc[i]
    result_df[f'{field_names[i]}_ST'] = st_df.iloc[i]

# add names of parameters
result_df.insert(0, 'Parameter', rock_physics['names'])

# save results
result_df.to_csv('..../sobol_sensitivity_results.csv', index = False)
print("The sobol sensitivty analysis results have save to sobol_sensitivity_results.csv")
#
#%%%%%
# plot sensitivty
# read csv table
# file path
file_path = '..../sobol_sensitivity_results.csv' 
# read table
data = pd.read_csv(file_path)
#
# select data — machanical field
labels = data['Parameter']
model1_results = data[['Sigma_11_ST', 'Sigma_22_ST', 'Sigma_12_ST']].mean(axis = 1)    # mean ST value of machanical fields
model2_results = data[['Sigma_11_S1', 'Sigma_22_S1', 'Sigma_12_S1']].mean(axis =1)     # mean S1 value of machanical fields 

# merge data to DataFrame and descending data according to Model1
#
df = pd.DataFrame({
    'Label': labels,
    'Model1': model1_results,
    'Model2': model2_results
})

# 按照Model1结果降序排序
df_sorted = df.sort_values(by='Model1', ascending = False)

# plot bars
x = np.arange(len(df_sorted))  # x labels
width = 0.35  # width of bars

# size of the bar plot
fig, ax1 = plt.subplots(figsize=(25, 2.5))

# bars of ST
bars1 = ax1.bar(x - width/2, df_sorted['Model1'], width, label='Model 1', color='b')
# bars of S1
bars2 = ax1.bar(x + width/2, df_sorted['Model2'], width, label='Model 2', color='g')

# add cumulative curve
# cumulatve percentage
cumulative_model1 = np.cumsum(df_sorted['Model1']) / np.sum(df_sorted['Model1']) * 100
cumulative_model2 = np.cumsum(df_sorted['Model2']) / np.sum(df_sorted['Model2']) * 100

# the second y label
ax2 = ax1.twinx()
line1, = ax2.plot(x, cumulative_model1, label='Cumulative Model 1', color='b', marker='o', linestyle='--', linewidth=2)
line2, = ax2.plot(x, cumulative_model2, label='Cumulative Model 2', color='g', marker='o', linestyle='--', linewidth=2)

# set legend of the plot
ax1.set_xlabel('Labels')
ax1.set_ylabel('Results')
ax2.set_ylabel('Cumulative Percentage (%)')

# show labels of x corrdinate
ax1.set_xticks(x)
ax1.set_xticklabels(df_sorted['Label'])

# merge legends
lines = [bars1, bars2, line1, line2]
labels = ['Model 1', 'Model 2', 'Cumulative Model 1', 'Cumulative Model 2']
ax1.legend(lines, labels, loc='best', bbox_to_anchor=(1, 1))

# narrow the space of the two sides of corrdinate x
ax1.set_xlim(-0.5, len(df_sorted) - 0.8)  

# show and save the plot
# save to EMF formate
plt.savefig('output_chart.pdf', format='pdf')
plt.show()